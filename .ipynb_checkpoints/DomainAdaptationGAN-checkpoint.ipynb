{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c9951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy as CCELoss\n",
    "from tensorflow.keras.losses import BinaryCrossentropy as BCELoss\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5f517",
   "metadata": {},
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3268f",
   "metadata": {},
   "source": [
    "There are 2 Feature Extractor versions. One is from the original paper, however I do not see the purpose of using a Conv1d operation on image data. Therefore I also implemented a very basical Conv2D network with multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0251d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "conv_activation = 'sigmoid'\n",
    "clf_activation = 'relu'\n",
    "disc_activation = 'relu'\n",
    "input_shape = (28, 28, 1)\n",
    "hidden_len = 512\n",
    "feature_len = 256\n",
    "disc_len = 1024\n",
    "hidden_depth = 10\n",
    "kernel_size = 3\n",
    "num_conv_layers = 3\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "class FeatureExtractor(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.feature_extractor = keras.Sequential([\n",
    "            layers.Conv2D(filters=16, kernel_size=(2,2), activation='relu'),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.MaxPool2D(pool_size=(2,2)),\n",
    "            layers.Conv2D(filters=32, kernel_size=(2,2), activation='relu'),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.MaxPool2D(pool_size=(2,2)),\n",
    "            layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.MaxPool2D(pool_size=(2,2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "\n",
    "# class FeatureExtractor(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(FeatureExtractor, self).__init__()\n",
    "#         feature_extractor = keras.Sequential([])\n",
    "#         feature_extractor.add(layers.Reshape((-1, input_shape[0]**2, 3)))\n",
    "#         feature_extractor.add(layers.Dense(hidden_len))\n",
    "\n",
    "#         conv1d_layer = layers.Conv1D(filters=hidden_depth, kernel_size=kernel_size, activation=conv_activation)\n",
    "#         dropout_layer = layers.Dropout(dropout)\n",
    "#         for _ in range(num_conv_layers):\n",
    "#             feature_extractor.add(conv1d_layer)\n",
    "#             feature_extractor.add(dropout_layer)\n",
    "\n",
    "#         feature_extractor.add(layers.Flatten())\n",
    "#         feature_extractor.add(layers.Dense(feature_len, activation=conv_activation))\n",
    "#         self.feature_extractor = feature_extractor\n",
    "    \n",
    "#     def call(self, x):\n",
    "#         return self.feature_extractor(x)\n",
    "    \n",
    "    \n",
    "class Classifier(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.MLP = keras.Sequential([\n",
    "            layers.Dense(feature_len, activation=clf_activation),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.MLP(x)\n",
    "    \n",
    "    \n",
    "class Discriminator(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.MLP = keras.Sequential([\n",
    "            layers.Dense(disc_len, activation=disc_activation),\n",
    "            layers.Dense(disc_len, activation=disc_activation),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.MLP(x)\n",
    "    \n",
    "    \n",
    "class Backbone(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Backbone, self).__init__()\n",
    "        self.f = FeatureExtractor()\n",
    "        self.clf = Classifier()\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.clf(self.f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e228f",
   "metadata": {},
   "source": [
    "# Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5d0ae",
   "metadata": {},
   "source": [
    "The Training is implemented in 2 Stages, like in the paper. There is an Option to use reversed_gradients like in the paper, or do a regular GAN loss as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6426ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -dy\n",
    "    return y, custom_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a002092",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d7a44909b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Trainer(tf.keras.Model):\n",
    "    def __init__(self, args):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.logdir = args.logdir  \n",
    "        self.writer = tf.summary.create_file_writer(self.logdir)\n",
    "        self.eval_every = args.eval_every\n",
    "        self.epochs_stage_1 = args.epochs_stage_1\n",
    "        self.epochs_stage_2 = args.epochs_stage_2\n",
    "        \n",
    "        \n",
    "        self.model_1 = Backbone()\n",
    "        self.model_2 = Backbone()\n",
    "        self.discriminator = Discriminator()\n",
    "        \n",
    "        \n",
    "        self.lr = args.lr\n",
    "        self.optimizer_pre = Adam(self.lr)\n",
    "        self.optimizer_gen = Adam(self.lr)\n",
    "        self.optimizer_disc = Adam(self.lr)\n",
    "        self.optimizer_gen_and_clf = Adam(self.lr)\n",
    "        \n",
    "        self.reverse_gradients = args.reverse_gradients\n",
    "\n",
    "    \n",
    "    def train_stage_1(self, train_dataset, eval_dataset):\n",
    "        self.model_1.trainable = True\n",
    "        self.model_2.trainable = False\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        \n",
    "        self.model_1.compile(\n",
    "            optimizer = self.optimizer_pre,\n",
    "            loss = CCELoss()\n",
    "        )\n",
    "        self.model_1.fit(\n",
    "            train_dataset,\n",
    "            validation_data=eval_dataset,\n",
    "            epochs=self.epochs_stage_1\n",
    "        )\n",
    "    \n",
    "    def train_stage_2(self, train_dataset, eval_dataset):\n",
    "        model.model_1.trainable = False\n",
    "        model.model_2.trainable = True\n",
    "        self.discriminator.trainable = True\n",
    "                \n",
    "        def train_one_batch(x_source, y_source, x_target, step, training):\n",
    "            with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc, tf.GradientTape() as tape_gen_and_clf:\n",
    "                # feature extraction\n",
    "                f_source_pretrained = self.model_1.f(x_source, training=False)\n",
    "                f_source_aligned    = self.model_2.f(x_source, training=training)\n",
    "                f_target            = self.model_2.f(x_target, training=training)\n",
    "                \n",
    "\n",
    "                # step b: classification loss\n",
    "                y_pred = self.model_2.clf(f_source_aligned, training=training)\n",
    "                L_clf = CCELoss()(y_source, y_pred)\n",
    "\n",
    "                # step c: consistency loss\n",
    "                L_c = tf.reduce_mean(tf.abs(f_source_pretrained - f_source_aligned), axis=-1)\n",
    "\n",
    "                # step d: reverse gradient and adversarial alignment loss\n",
    "                if self.reverse_gradients:\n",
    "                    f_source_aligned = grad_reverse(f_source_aligned)\n",
    "                    f_target         = grad_reverse(f_target)\n",
    "                    loss_sign = -1\n",
    "                else:\n",
    "                    loss_sign = 1\n",
    "                \n",
    "                D_source_pretrained = self.discriminator(f_source_pretrained, training=training)\n",
    "                D_target            = self.discriminator(f_target, training=training)\n",
    "                \n",
    "            \n",
    "                L_d_disc = BCELoss()(tf.ones_like(D_source_pretrained), D_source_pretrained) \\\n",
    "                         + BCELoss()(tf.zeros_like(D_target), D_target) \n",
    "                L_d_gen = BCELoss()(tf.zeros_like(D_target), D_target) \n",
    "                \n",
    "                # step e: overall loss and update\n",
    "                loss_gen_and_clf = tf.reduce_mean(L_clf)\n",
    "                loss_gen = tf.reduce_mean(-loss_sign*L_d_gen + L_c)\n",
    "                loss_disc = tf.reduce_mean(loss_sign*L_d_disc)\n",
    "                \n",
    "                with self.writer.as_default(step=step):\n",
    "                    tag = 'train' if training else 'eval'\n",
    "                    tf.summary.scalar(f\"{tag}/L_clf\", tf.reduce_mean(L_clf))\n",
    "                    tf.summary.scalar(f\"{tag}/L_c\", tf.reduce_mean(L_c))\n",
    "                    tf.summary.scalar(f\"{tag}/L_d\", tf.reduce_mean(L_d_gen))\n",
    "                    tf.summary.scalar(f\"{tag}/p(Y=source | x_source)\", tf.reduce_mean(D_source_pretrained))\n",
    "                    tf.summary.scalar(f\"{tag}/p(Y=source | x_target)\", tf.reduce_mean(D_target))\n",
    "\n",
    "            \n",
    "            if training:\n",
    "                weights_gen_and_clf = self.model_2.trainable_variables \n",
    "                gradients_gen_and_clf = tape_gen_and_clf.gradient(loss_gen_and_clf, weights_gen_and_clf)\n",
    "                self.optimizer_gen_and_clf.apply_gradients(zip(gradients_gen_and_clf, weights_gen_and_clf))\n",
    "\n",
    "                weights_gen = self.model_2.f.trainable_variables \n",
    "                gradients_gen = tape_gen.gradient(loss_gen, weights_gen)\n",
    "                self.optimizer_gen.apply_gradients(zip(gradients_gen, weights_gen))\n",
    "\n",
    "                weights_disc = self.discriminator.trainable_variables \n",
    "                gradients_disc = tape_disc.gradient(loss_disc, weights_disc)\n",
    "                self.optimizer_disc.apply_gradients(zip(gradients_disc, weights_disc))\n",
    "        \n",
    "        train_batch_count = 0\n",
    "        eval_batch_count = 0\n",
    "        for epoch in tqdm(range(self.epochs_stage_2), desc='epochs', leave=True):\n",
    "            for batch in tqdm(train_dataset, desc='train batches', leave=False):\n",
    "                x_source, y_source, x_target = batch\n",
    "                train_one_batch(x_source, y_source, x_target, step=train_batch_count, training=True)\n",
    "                train_batch_count += 1\n",
    "                \n",
    "            if epoch % self.eval_every == 0:\n",
    "                for step, batch in tqdm(enumerate(eval_dataset), desc='eval batches', leave=False):\n",
    "                    x_source, y_source, x_target = batch\n",
    "                    train_one_batch(x_source, y_source, x_target, step=eval_batch_count, training=False)\n",
    "                    eval_batch_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c6955",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efeb934",
   "metadata": {},
   "source": [
    "The datasat preprocessing is copyied from the DANN experiment. If you want to replicate the training, please download the dataset from this link: https://github.com/sghoshjr/tf-dann/releases/download/v1.0.0/mnistm.h5\n",
    "\n",
    "And set the variable:  \"MNIST_M_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "219cc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_M_PATH = 'mnistm.h5'\n",
    "BATCH_SIZE = 64\n",
    "CHANNELS = 3\n",
    "NUM_SAMPLES = 10000\n",
    "VAL_SET = 0.2\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    #Load MNIST Data (Source)\n",
    "    (mnist_train_x, mnist_train_y), (mnist_test_x, mnist_test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    #Convert to 3 Channel and One_hot labels\n",
    "    mnist_train_x, mnist_test_x = mnist_train_x.reshape((60000, 28, 28, 1)), mnist_test_x.reshape((10000, 28, 28, 1))\n",
    "    mnist_train_x, mnist_test_x = mnist_train_x[:NUM_SAMPLES], mnist_test_x[:int(NUM_SAMPLES*VAL_SET)]\n",
    "    mnist_train_y, mnist_test_y = mnist_train_y[:NUM_SAMPLES], mnist_test_y[:int(NUM_SAMPLES*VAL_SET)]\n",
    "    mnist_train_x, mnist_test_x = mnist_train_x / 255.0, mnist_test_x / 255.0\n",
    "    mnist_train_x, mnist_test_x = mnist_train_x.astype('float32'), mnist_test_x.astype('float32')\n",
    "\n",
    "    mnist_train_x = np.repeat(mnist_train_x, CHANNELS, axis=3)\n",
    "    mnist_test_x = np.repeat(mnist_test_x, CHANNELS, axis=3)\n",
    "    mnist_train_y = tf.one_hot(mnist_train_y, depth=10)\n",
    "    mnist_test_y = tf.one_hot(mnist_test_y, depth=10)\n",
    "\n",
    "\n",
    "\n",
    "    #Load MNIST-M [Target]\n",
    "\n",
    "    with h5py.File(MNIST_M_PATH, 'r') as mnist_m:\n",
    "        mnist_m_train_x, mnist_m_test_x = mnist_m['train']['X'][()], mnist_m['test']['X'][()]\n",
    "\n",
    "    mnist_m_train_x, mnist_m_test_x = mnist_m_train_x[:NUM_SAMPLES], mnist_m_test_x[:int(NUM_SAMPLES*VAL_SET)]\n",
    "    mnist_m_train_x, mnist_m_test_x = mnist_m_train_x / 255.0, mnist_m_test_x / 255.0\n",
    "    mnist_m_train_x, mnist_m_test_x = mnist_m_train_x.astype('float32'), mnist_m_test_x.astype('float32')\n",
    "    mnist_m_train_y, mnist_m_test_y = mnist_train_y, mnist_test_y\n",
    "\n",
    "    ds_stage_1_train = Dataset.from_tensor_slices((mnist_train_x, mnist_train_y)).batch(BATCH_SIZE)\n",
    "    ds_stage_1_test = Dataset.from_tensor_slices((mnist_test_x, mnist_test_y)).batch(BATCH_SIZE)\n",
    "    ds_stage_2_train = Dataset.from_tensor_slices((mnist_train_x, mnist_train_y, mnist_m_train_x)).batch(BATCH_SIZE)\n",
    "    ds_stage_2_test = Dataset.from_tensor_slices((mnist_test_x, mnist_test_y, mnist_m_test_x)).batch(BATCH_SIZE)\n",
    "    \n",
    "    return ds_stage_1_train, ds_stage_1_test, ds_stage_2_train, ds_stage_2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad849e",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e4620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    pass\n",
    "args = Arguments()\n",
    "args.lr = 0.01\n",
    "args.logdir = 'logs'\n",
    "args.eval_every = 1\n",
    "args.epochs_stage_1 = 3\n",
    "args.epochs_stage_2 = 3\n",
    "args.reverse_gradients = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939a601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "157/157 [==============================] - 18s 106ms/step - loss: 1.6613 - val_loss: 0.9778\n",
      "Epoch 2/3\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 0.8907 - val_loss: 1.1217\n",
      "Epoch 3/3\n",
      "157/157 [==============================] - 19s 119ms/step - loss: 0.7250 - val_loss: 0.9192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647c6bc90e3f41dea341fbb5b34809a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ds_stage_1_train, ds_stage_1_test, ds_stage_2_train, ds_stage_2_test = prepare_data()\n",
    "    model = Trainer(args)\n",
    "    model.train_stage_1(ds_stage_1_train, ds_stage_1_test)\n",
    "    model.train_stage_2(ds_stage_2_train, ds_stage_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74434374",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\n",
      "TensorBoard 2.4.0 at http://localhost:6006/ (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94ea67",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c70db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 6\n",
    "fig, ax = plt.subplots(num_samples, 2, figsize=(20,20))\n",
    "batch = next(iter(ds_stage_2_train))\n",
    "x_source, y_source, x_target = batch\n",
    "y_pred_pre = model.model_1(x_target)\n",
    "y_pred = model.model_2(x_target)\n",
    "\n",
    "L_clf = CCELoss()(y_source, y_pred)\n",
    "L_clf_pre = CCELoss()(y_source, y_pred_pre)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    ax[i, 0].imshow(x_source[i])\n",
    "    ax[i, 1].imshow(x_target[i])\n",
    "    ax[i, 1].set_title(f'Prediction: {y_pred.numpy().argmax(axis=-1)[i]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
